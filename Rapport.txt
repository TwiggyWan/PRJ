Introduction
	Contexte du projet & Présentation du drone
		La société parisienne Parrot commercialise depuis 2010 un quadricoptère grand public, l'AR.Drone.
		Conçu pour la production à grande échelle et pour une utilisation grand public, 
		il est contrôlable directement depuis un smartphone. Sorti en 2012, l'AR.Drone 2.0 embarque
		deux caméras, l'une HD 720p filmant l'avant, l'autre QVGA (320x240) filmant le sol. 
		Il est capable d'afficher en temps réel les images sur le smartphone servant de télécommande via 
		Wi-Fi. 

	Présentation du problème
		Pour permettre un pilotage aisé, le drone est asservi à l'aide d'une IMU (Inertial Measurement Unit),
		de la caméra QVGA et de capteurs d'altitude. Il est capable de maintenir une altitude constante,
		une assiette horizontale et une position statique.
		
		Cependant, la méthode utilisée pour garantir une position statique présente des défauts. Elle se base sur 
		de la reconnaissance de formes au sol à l'aide de la caméra QVGA. Seulement, il arrive que le drone ait
		à évoluer sur un sol uni en intérieur, comme de la moquette ou un entrepot. De plus lorsque la luminosité
		est mauvaise, la caméra QVGA est très sensible au bruit numérique et il n'est plus possible de distinguer 
		quoi que ce soit.
		
	Objectifs	
		Nous souhaitons donc remédier à ce problème en utilisant la caméra frontale commme source de données
		pour réaliser un asservissement visuel. Le drone devra pouvoir assurer son immobilité en se repérant
		par rapport aux divers objets du décor devant lui.
		Par la suite, nous souhaitons également le rendre capable d'aller d'un point A à un point B en lui fournissant
		une image de référence, validant ainsi l'asservissement réalisé pour une utilisation future au sein d'autres 
		systèmes.
		
		
I. Cahier des charges
	Besoins
	Rendre le drone conscient de sa dérive
	Récupérer le flux vidéo
	Récupérer les données de l'IMU
	Se positionner par rapport à une image de référence pour rester statique
	Se positionner par rapport à une image de référence pour aller d'un endroit à un autre,
	cet autre endroit doit être vu depuis la position initiale
	Laisser fonctionner l'algorithme parrot quand il le peut et faire fonctionner
	le notre dans le cas contraire
	
		Pour réaliser notre objectif principal, nous pouvons lister un certain nombre de besoins :
			Le drone doit avoir conscience de sa dérive;
			Il doit pouvoir se positionner par rapport à une image de référence pour rester statique;
			Il doit pouvoir se positionner par rapport à une image de référence 
				pour aller d'un endroit à un autre;
			Cet endroit en question doit être vu depuis la position de départ.
			
		Pour les réaliser, nous devons :
			Récupérer le flux vidéo émis par le drone;
			Récupérer les données de l'IMU et des capteurs d'altitude;
			Extraire des images des données permettant au drone de se positionner (points d'intérêt);
			Réaliser un correcteur pour asservir le drone à partir des données de l'image.
				
	Contraintes
	1sec/xFPS = temps de calcul max pour chaque échantillon
	asservissement réalisé uniquement avec les données provenant du drone
	pas de stéréovision avec les deux caméras -> la profondeur de champ n'est pas directement accessible
	traitement déporté à un ordinateur en liaison wifi avec le drone
	
		Un certain nombre de contraintes nous sont imposées :
		La caméra fonctionnant à 30 FPS, il est nécessaire que tout le traitement d'un échantillon prenne moins
			de 1000/30 = 33 ms;
		L'asservissement ne devra être réalisé qu'avec les données provenant des capteurs internes;
		Le drone n'étant pas équipé de caméras 3D ou d'un système de stéréovision, la profondeur du champ 
			de ce qui est vu n'est pas directement accessible
		Le traitement des données se fera sur un ordinateur en liaison Wi-Fi avec le drone
	
	
	
II. Etat de l'art

		La vision par ordinateur est un sujet vaste et étudié depuis plusieurs décennies avec des applications aussi
	bien en informatique qu'en robotique. Les images peuvent notamment servir à faire
	naviguer des robots à partir des informations contenues dans les images.
	Il existe trois façons de faire [13]:
		Avec une carte préétablie,
		En construisant une carte de manière incrémentale,
		Sans carte mais avec de la reconnaissance et du traçage d'objets ou de points d'intérêt.
	
		Les applications en robotique de ces techniques sont nombreuses car cette technique permet à un
	robot de se guider par rapport au décor qui l'entoure et les données visuelles se recoupent facilement avec celles 
	d'autres capteurs comme le GPS.
	
		Dans les deux premiers cas, l'asservissement est réalisé à partir de la carte dont le système dispose. Pour la dernière option, on parle
	d'asservissement visuel. Dans ce domaine, de nombreux travaux ont déjà été menés, les concepts géométriques utilisés 
	pour les lois de commande étant bien établis. 
	On distingue deux approches, l'une basée sur les données directement accessibles depuis les images (Image Based Visual Servoing), 
	l'autre qui repose sur des données 3D (Position Based Visual Servoing)[5].

		Dans[4] et [2], des données visuelles d'intérêt sont d'abord extraites des images via de la détection
	de contours puis recoupées avec les données d'une IMU et d'un GPS pour mettre en oeuvre la loi de commande. 
	
		Dans [9], Une solution basée sur l'utilisation d'une seule caméra et d'une IMU est proposée pour déterminer le mouvement
	d'un drone volant en un nombre minimum d'itérations grâce à un algorithme RANSAC(random sample consensus) 
	[8] modifié, et l'application du concept d'odométrie visuelle. En effet, il est possible de déterminer le
	mouvement de la caméra (et donc du système) en examinant les images successives produites au cours du 
	déplacement.
	
		Pour la construction incrémentale de carte, de nombreux travaux faisant également appel à
	l'odométrie visuelle ont également été menés [10]. Ils sont souvent couplés à une famille particulière
	d'algorithmes, les SLAM (Simultaneous Localization and Mapping)[7]. Dans [14], un algorithme SLAM est
	implanté dans l'AR.Drone en utilisant la caméra du bas et le capteur à ultrasons pour construire la carte 
	tridimensionnelle. On trouve d'ailleurs des programmes de shape from motion (SFM) disponibles gratuitement
	pour faire de la reconstruction 3D à partir d'images [18]
	
	
		En odométrie visuelle, des méthodes de filtrage et de détection d'erreurs sont nécessaires afin de 
	garantir la cohérence des données. Pour ce faire l'algorithme RANSAC peut être utilisé mais le filtre de Kalman
	est également une alternative.
	
		Il existe également des travaux menés sur  l'asservissement visuel en deux dimensions uniquement. 
	Dans [17], un tel asservissement est mis en oeuvre en utilisant uniquement les données provenant de l'image
	actuelle et de l'image de référence, sans avoir besoin de la profondeur, pour calculer la loi de commande nécessaire.
	Les six degrés de liberté d'un robot dans l'espace sont ainsi contrôlables.
	

		Pour résumer, il est nécessaire d'avoir une idée de la position du drone à un instant t pour être capable
	de l'asservir à l'aide d'une loi de commande. La difficulté réside dans l'acquisition et le traitement des données
	qui permettent d'extraire les informations nécessaires à la loi de commande. Les techniques d'estimation de pose
	pour en extraire des données 3D sont les plus répandues même s'il reste possible de faire un asservissement visuel
	à partir de données 2D.
	

III. Solution choisie
	Choix d'un algorithme
	Schéma synoptique de l'asservissement
	
	
IV. Développement
	Outils utilisés
		Parrot API
		Open CV
		IDE/Compilateurs
			
	Tests de vol du drone
	Tests de capture de vidéos
	Implémentation de l'algorithme
	
Conclusion
	Perspectives