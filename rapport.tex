
\documentclass[10pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[francais]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Antoine MEUNIER, Anis SEMMAR}

\begin{document}

\part*{Asservissement visuel d'un drone}

\chapter {Travail préliminaire}
	
\section{Introduction}


 		La société parisienne Parrot commercialise depuis 2010 un quadricoptère grand public, l'AR.Drone. Conçu pour la production à grande échelle et pour une utilisation grand public, il est contrôlable directement depuis un smartphone. Sorti en 2012, l'AR.Drone 2.0 embarque deux caméras, l'une HD 720p filmant l'avant, l'autre QVGA (320x240) filmant le sol. Il est capable d'afficher en temps réel les images sur le smartphone servant de télécommande via Wi-Fi. 


		Pour permettre un pilotage aisé, le drone est asservi à l'aide d'une IMU (Inertial Measurement Unit), de la caméra QVGA et de capteurs d'altitude. Il est capable de maintenir une altitude constante, une assiette horizontale et une position statique. Cependant, la méthode utilisée pour garantir une position statique présente des défauts. Elle se base sur de la reconnaissance de formes au sol à l'aide de la caméra QVGA. Seulement, il arrive que le drone ait à évoluer sur un sol uni en intérieur, comme de la moquette ou un entrepôt. De plus lorsque la luminosité est mauvaise, la caméra QVGA est très sensible au bruit numérique et il n'est plus possible de distinguer quoi que ce soit.
		

		Nous souhaitons donc remédier à ce problème en utilisant la caméra frontale comme source de données pour réaliser un asservissement visuel. Le drone devra pouvoir assurer son immobilité en se repérant par rapport aux divers objets du décor devant lui. Par la suite, nous souhaitons également le rendre capable d'aller d'un point A à un point B en lui fournissant une image de référence, validant ainsi l'asservissement réalisé pour une utilisation future au sein d'autres systèmes.
		

				
		Pour réaliser notre objectif principal, nous pouvons lister un certain nombre de besoins :
		\begin{itemize}
		\item Le drone doit avoir conscience de sa dérive;
		\item Il doit pouvoir se positionner par rapport à une image de référence 			pour rester statique;
		\item Il doit pouvoir se positionner par rapport à une image de référence 
			pour aller d'un endroit à un autre;
		\item Cet endroit en question doit être vu depuis la position de départ.
		\end{itemize}
						
		Pour les réaliser, nous devons :
		\begin{itemize}
		\item Récupérer le flux vidéo émis par le drone;
		\item Récupérer les données de l'IMU et des capteurs d'altitude;
		\item Extraire des images des données permettant au drone de se 						positionner (points d'intérêt);
		\item Réaliser un correcteur pour asservir le drone à partir des données 				de l'image.
		\end{itemize}
			
			
				
		Un certain nombre de contraintes nous sont imposées :
		
		\begin{itemize}
		\item La caméra fonctionnant à 30 FPS, il est nécessaire que tout le 					traitement d'un échantillon prenne moins de 1000/30 = 33 ms;
		\item L'asservissement ne devra être réalisé qu'avec les données 						provenant des capteurs internes;
		\item Le drone n'étant pas équipé de caméras 3D ou d'un système de 						stéréo-vision, la profondeur du champ de ce qui est vu n'est pas 					directement accessible;
		\item Le traitement des données se fera sur un ordinateur en liaison Wi-				Fi avec le drone.
		\end{itemize}
	
\newpage		
		
			
		
	
	
\section{Etat de l'art}
		

		La vision par ordinateur est un sujet vaste et étudié depuis plusieurs décennies avec des applications aussi bien en informatique qu'en robotique. Les images peuvent notamment servir à faire naviguer des robots à partir des informations contenues dans les images.\newline
		
		
	Il existe trois façons de faire [13]:
	
\begin{itemize}
\item Avec une carte préétablie,
\item En construisant une carte de manière incrémentale,
\item Sans carte mais avec de la reconnaissance et du traçage d'objets ou de points d'intérêt.\newline
\end{itemize}


			
	
		Les applications en robotique de ces techniques sont nombreuses car cette technique permet à un robot de se guider par rapport au décor qui l'entoure et les données visuelles se recoupent facilement avec celles d'autres capteurs comme le GPS.
	
		Dans les deux premiers cas, l'asservissement est réalisé à partir de la carte dont le système dispose. Pour la dernière option, on parle d'asservissement visuel. Dans ce domaine, de nombreux travaux ont déjà été menés, les concepts géométriques utilisés pour les lois de commande étant bien établis. 
		
	On distingue deux approches, l'une basée sur les données directement accessibles depuis les images (Image Based Visual Servoing), l'autre qui repose sur des données 3D (Position Based Visual Servoing)[5].

		Dans[4] et [2], des données visuelles d'intérêt sont d'abord extraites des images via de la détection de contours puis recoupées avec les données d'une IMU et d'un GPS pour mettre en oeuvre la loi de commande. 
	
		Dans [9], Une solution basée sur l'utilisation d'une seule caméra et d'une IMU est proposée pour déterminer le mouvement d'un drone volant en un nombre minimum d'itérations grâce à un algorithme RANSAC(random sample consensus) 
[8] modifié, et l'application du concept d'odométrie visuelle. En effet, il est possible de déterminer le mouvement de la caméra (et donc du système) en examinant les images successives produites au cours du déplacement.
	
		Pour la construction incrémentale de carte, de nombreux travaux faisant également appel à l'odométrie visuelle ont également été menés [10]. Ils sont souvent couplés à une famille particulière d'algorithmes, les SLAM (Simultaneous Localization and Mapping)[7]. Dans [14], un algorithme SLAM est implanté dans l'AR.Drone en utilisant la caméra du bas et le capteur à ultrasons pour construire la carte tridimensionnelle. On trouve d'ailleurs des programmes de shape from motion (SFM) disponibles gratuitement pour faire de la reconstruction 3D à partir d'images[18].
	
		En odométrie visuelle, des méthodes de filtrage et de détection d'erreurs sont nécessaires afin de garantir la cohérence des données. Pour ce faire l'algorithme RANSAC peut être utilisé mais le filtre de Kalman est également une alternative.
	
		Il existe également des travaux menés sur  l'asservissement visuel en deux dimensions uniquement. Dans [17], un tel asservissement est mis en oeuvre en utilisant uniquement les données provenant de l'image actuelle et de l'image de référence, sans avoir besoin de la profondeur, pour calculer la loi de commande nécessaire, les six degrés de liberté d'un robot dans l'espace étant ainsi contrôlables.
	
		Pour résumer, il est nécessaire d'avoir une idée de la position du drone à un instant t pour être capable de l'asservir à l'aide d'une loi de commande. La difficulté réside dans l'acquisition et le traitement des données qui permettent d'extraire les informations nécessaires à la loi de commande. Les techniques d'estimation de pose pour en extraire des données 3D sont les plus répandues même s'il reste possible de faire un asservissement visuel à partir de données 2D.
	
\newpage

\section{Solution choisie}

		Toutes les solutions préalablement évoquées possèdent leur propres avantages et inconvénients. Les contraintes de notre système excluent d'office les techniques faisant appel aux GPS ou à un système de plusieurs caméras. 
		
		Les techniques de reconstruction 3D et de SFM sont quant à elles utilisables par notre drone qui a un équipement similaire à celui utilisé dans [9]. Ces techniques étant basées sur de l'estimation de pose, c'est à dire l'estimation de paramètres 3D du système à partir des images, elles sont à la fois lourdes en traitement[8] et nécessitent une optimisation poussée pour contrer ce problème[9]. C'est pourquoi ces techniques sont le plus souvent mises en place au sein de projets plus longs comme des thèses. En effet les notions mathématiques mises en œuvre dépassent nos compétences actuelles et seraient un obstacle important dans un projet aussi court. Nous devons en effet nous montrer prudents sur le temps que nous prendra le développement effectif de l'application, et minimiser au maximum le temps passé à comprendre la théorie derrière le projet si nous voulons rendre un logiciel fini, optimisé et complet dans les temps.
		
		Ainsi, la méthode proposée en [17] offre des solutions face à nos problèmes précis. Nous pouvons nous passer de l'estimation de paramètres 3D et des données provenant de l'IMU et travailler uniquement avec  les données de l'image actuelle comparées à celles d'une image de référence. Ces informations servent à calculer les consignes translationnelles et rotationnelles en passant par la matrice d'homographie du système, le tout sans estimer de paramètres 3D et en étant robuste vis à vis des erreurs d'estimation des paramètres de la caméra.
		



	Schéma synoptique de l'asservissement
	

\section{Développement}	

IV. Développement
	Outils utilisés
		Parrot API
		Open CV
		IDE/Compilateurs
			
	Tests de vol du drone
	Tests de capture de vidéos
	Implémentation de l'algorithme
	
Conclusion
	Perspectives

\end{document}